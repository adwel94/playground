# Vision Safari Agent — 단계별 학습 계획

## 1. 기존 베이스 모델의 한계

**모델**: Qwen3-VL-2B-Thinking

- 도구 호출(function calling)은 어느 정도 가능한 상태
- 그러나 **이미지 인식 능력이 부족** — 10×10 그리드에서 이모지 동물의 위치/색상/종류를 정확히 식별하지 못함
- 도구 사용법은 학습 가능하지만, **시각적 이해 없이는 올바른 인자를 생성할 수 없음**

→ 도구 호출 학습(사파리 에이전트) 전에, **기초 시각 인식 능력을 먼저 강화**해야 함

## 2. 이모티콘 인식 게임으로 시각 인식 학습

**목적**: 이미지 → 구조화된 정보 추출 능력 강화

- `/emoji-recognition` 게임으로 학습 데이터 대량 생성
- 태스크: 10×10 뷰포트 스크린샷 → 동물 위치(x,y), 배경색, 종류 식별
- 8동물 × 8색상 = 64조합을 균등하게 커버
- 단일 도구(`update_notepad`)만 사용 → 도구 형식 학습 부담 최소화
- **정답 라벨이 프로그래밍적으로 항상 생성**되므로 teacher 모델 정답률에 의존하지 않음

**학습 데이터 포맷**: Qwen3-VL conversation format (system → user[image+text] → assistant[tool_call] → tool[result])

**목표**: 200~300+ 라운드 수집 → LoRA fine-tuning → **이모지 인식 어댑터 (V1)**

## 3. 인식 능력 위에 사파리 에이전트 학습

**학습 순서**:

```
Qwen3-VL-2B-Thinking (베이스)
  ↓  이모지 인식 LoRA 학습
  ↓  어댑터 머지 → 새 베이스
  ↓
Qwen3-VL-2B + 인식능력 (강화된 베이스)
  ↓  사파리 에이전트 LoRA 학습 (멀티턴 도구 사용, 전략 판단)
  ↓
Vision Safari Agent (최종 모델)
```

**단계별 기대 효과**:

| 단계 | 학습 내용 | 기대 효과 |
|------|----------|----------|
| 인식 게임 LoRA | 좌표 체계, 이모지 식별, 색상 구분 | 시각 → 텍스트 변환 정확도 향상 |
| 사파리 에이전트 LoRA | 멀티턴 도구 호출, 이동/포획 전략 | 인식 능력 기반 위에 행동 전략 학습 |

**핵심 이점**: 인식을 먼저 학습한 모델은 사파리 에이전트 학습 시 시각 정보를 정확히 이해한 상태에서 전략을 학습하므로, 베이스 모델에서 바로 에이전트 학습하는 것보다 효과적