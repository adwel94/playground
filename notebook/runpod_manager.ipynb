{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# RunPod GPU ê´€ë¦¬ ë…¸íŠ¸ë¶\n",
    "\n",
    "REST API ì§ì ‘ í˜¸ì¶œ ë°©ì‹ (serverless-mlops íŒ¨í„´).\n",
    "GPUType enumìœ¼ë¡œ íƒ€ì… ì•ˆì „í•˜ê²Œ GPUë¥¼ ì§€ì •í•˜ê³ , pandas DataFrameìœ¼ë¡œ Pod ëª©ë¡ì„ í™•ì¸í•œë‹¤.\n",
    "\n",
    "| GPU       | VRAM | ~$/hr | ìš©ë„          |\n",
    "|-----------|------|------|-------------|\n",
    "| RTX 4090  | 24GB | 0.39 | 2B ëª¨ë¸ í•™ìŠµ/ì¶”ë¡  |\n",
    "| L40S      | 48GB | 0.74 | 7B ëª¨ë¸ ì„œë¹™    |\n",
    "| A100 80GB | 80GB | 1.64 | ëŒ€í˜• ëª¨ë¸       |"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T15:37:58.815443600Z",
     "start_time": "2026-02-16T15:37:58.741870300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "\n",
    "from utils.runpod_client import *\n",
    "\n",
    "print(\"RunPod REST í´ë¼ì´ì–¸íŠ¸ ë¡œë“œ ì™„ë£Œ\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunPod REST í´ë¼ì´ì–¸íŠ¸ ë¡œë“œ ì™„ë£Œ\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ì…€ 2: Pod ëª©ë¡ ì¡°íšŒ â†’ DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "pod_list = pods()\n",
    "if pod_list:\n",
    "    df = pd.DataFrame(pod_list)\n",
    "    display(df)\n",
    "else:\n",
    "    print(\"ì‹¤í–‰ ì¤‘ì¸ Podì´ ì—†ìŠµë‹ˆë‹¤.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ì…€ 3: Pod ìƒì„± (GPUType enum ì‚¬ìš©)\n",
    "pod_id = create(\n",
    "    name=\"my-workspace\",\n",
    "    gpu_id=GPUType.NVIDIA_GEFORCE_RTX_4090,\n",
    "    gpu_count=1,\n",
    "    volume=50,\n",
    "    image_name=\"runpod/pytorch:2.4.0-py3.11-cuda12.4.1-devel-ubuntu22.04\",\n",
    ")\n",
    "print(f\"ìƒì„±ëœ Pod ID: {pod_id}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ì…€ 4: íŠ¹ì • Pod ìƒíƒœ ì¡°íšŒ\n",
    "# pod_id = \"POD_ID\"\n",
    "pod_info = pod(pod_id)\n",
    "pod_info"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ì…€ 5: Pod ì‚­ì œ\n",
    "# pod_id = \"POD_ID\"\n",
    "result = delete(\"6bo9b41ss21l6t\")\n",
    "print(f\"ì‚­ì œ ê²°ê³¼: {result}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ì…€ 6: VLM ì„œë¹™ Pod ìƒì„±\n",
    "vlm_pod_id = create_from_template(\n",
    "    name=\"qwen3-vl-4b-serving\",\n",
    "    gpu_id=GPUType.NVIDIA_GEFORCE_RTX_5080,\n",
    ")\n",
    "print(f\"VLM Pod ID: {vlm_pod_id}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-16T15:38:23.344291700Z",
     "start_time": "2026-02-16T15:38:02.397995100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ì…€ 7: vLLM ì—°ê²° í…ŒìŠ¤íŠ¸\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "VLLM_BASE_URL = \"https://api.runpod.ai/v2/qdwwc4uin801j6/openai/v1\" # ì˜ˆ: http://your-pod-ip:8000/v1\n",
    "MODEL_NAME = \"qwen/qwen3-vl-4b-thinking\"\n",
    "\n",
    "if VLLM_BASE_URL != \"ì—¬ê¸°ì—_URLì„_ì…ë ¥í•˜ì„¸ìš”\":\n",
    "    llm = ChatOpenAI(\n",
    "        base_url=VLLM_BASE_URL,\n",
    "        model=MODEL_NAME,\n",
    "        temperature=0,\n",
    "        api_key=os.getenv(\"RUNPOD_API_KEY\")\n",
    "    )\n",
    "\n",
    "    response = llm.invoke([\n",
    "        HumanMessage(content=\"ì•ˆë…•! ë„ˆëŠ” ì–´ë–¤ ëª¨ë¸ì´ë‹ˆ? ë„êµ¬ í˜¸ì¶œì´ ê°€ëŠ¥í•˜ë‹ˆ?\")\n",
    "    ])\n",
    "    print(\"\\n[ì‘ë‹µ ê²°ê³¼]\")\n",
    "    print(response.content)\n",
    "else:\n",
    "    print(\"VLLM_BASE_URLì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ì‘ë‹µ ê²°ê³¼]\n",
      "\n",
      "\n",
      "ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ì•Œë¦¬ë°”ë°” ê·¸ë£¹ ì‚°í•˜ Tongyi ì‹¤í—˜ì‹¤ì—ì„œ ê°œë°œí•œ ì´ˆëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì¸ **Qwen**ì…ë‹ˆë‹¤. \n",
      "\n",
      "### ì£¼ìš” ê¸°ëŠ¥:\n",
      "- **ì§ˆë¬¸ ë‹µë³€**: ë‹¤ì–‘í•œ ì£¼ì œì— ëŒ€í•œ ì •ë³´ë¥¼ ì œê³µí•˜ê±°ë‚˜ ë…¼ë¦¬ì  ì¶”ë¡ ì„ ë„ì™€ë“œë¦½ë‹ˆë‹¤.\n",
      "- **í…ìŠ¤íŠ¸ ìƒì„±**: ì´ì•¼ê¸°, ê³µë¬¸ì„œ, ì‹œë‚˜ë¦¬ì˜¤, ì´ë©”ì¼ ë“± ë‹¤ì–‘í•œ í˜•ì‹ì˜ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "- **ì½”ë“œ ì‘ì„±**: ì—¬ëŸ¬ í”„ë¡œê·¸ë˜ë° ì–¸ì–´(ì˜ˆ: Python, JavaScript, Java ë“±)ë¥¼ ì§€ì›í•˜ë©°, ë¬¸ì œ í•´ê²°ì„ ë„ì™€ì¤ë‹ˆë‹¤.\n",
      "- **ë‹¤êµ­ì–´ ì§€ì›**: í•œêµ­ì–´, ì˜ì–´, ì¤‘êµ­ì–´, ì¼ë³¸ì–´ ë“± 100ê°œ ì´ìƒì˜ ì–¸ì–´ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.\n",
      "\n",
      "### ë„êµ¬ í˜¸ì¶œ ê°€ëŠ¥ ì—¬ë¶€:\n",
      "ì €ëŠ” **ì‹¤ì‹œê°„ ì™¸ë¶€ ë„êµ¬ë‚˜ APIì— ì§ì ‘ ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤**. í•˜ì§€ë§Œ, ë„êµ¬ í˜¸ì¶œì´ í•„ìš”í•œ ì‘ì—…ì„ ë„ì™€ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´:\n",
      "- íŠ¹ì • APIë¥¼ ì‚¬ìš©í•´ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ë•Œ, **ì½”ë“œ ì˜ˆì‹œ**ë¥¼ ì œê³µí•´ ì£¼ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "- ë„êµ¬ì˜ ì‚¬ìš© ë°©ë²•ì„ ì„¤ëª…í•´ ì£¼ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "- ë„êµ¬ì™€ì˜ ì—°ë™ì„ ìœ„í•œ **êµ¬ì²´ì ì¸ ë‹¨ê³„**ë¥¼ ì•ˆë‚´í•´ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "ì˜ˆë¥¼ ë“¤ì–´, \"Google Maps APIë¥¼ ì‚¬ìš©í•´ ìœ„ì¹˜ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ëŠ” ë°©ë²•\"ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´, ì œê°€ ì½”ë“œ ì˜ˆì‹œë‚˜ ë‹¨ê³„ë¥¼ ì•Œë ¤ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
      "\n",
      "í˜¹ì‹œ êµ¬ì²´ì ì¸ ë„êµ¬ë‚˜ ì‘ì—…ì´ ìˆìœ¼ì‹ ê°€ìš”? í•¨ê»˜ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤! ğŸ˜Š\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
