{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이모지 LoRA → 베이스 모델 머지\n",
    "\n",
    "이모지 인식 LoRA 가중치를 베이스 모델에 합쳐 단일 모델로 만든다.  \n",
    "사파리 에이전트 LoRA 학습 시 이모지 인식 능력이 내장된 베이스 위에서 학습하기 위함.\n",
    "\n",
    "| 항목 | 값 |\n",
    "|------|----|\n",
    "| 베이스 모델 | `Qwen/Qwen3-VL-2B-Thinking` |\n",
    "| LoRA 어댑터 | `adwel94/vision-emoji-recognition-lora` |\n",
    "| 머지 결과 | `adwel94/Qwen3-VL-2B-Emoji-Base` |\n",
    "\n",
    "CPU에서 실행 가능 (2B bf16 ≈ 4GB, RAM 16GB)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-27T06:03:48.116360Z",
     "start_time": "2026-02-27T06:03:35.171210300Z"
    }
   },
   "source": "from transformers import Qwen3VLForConditionalGeneration, AutoProcessor\nfrom peft import PeftModel\nimport torch\n\nBASE_MODEL = \"Qwen/Qwen3-VL-2B-Thinking\"\nLORA_ADAPTER = \"adwel94/vision-emoji-recognition-lora\"\nMERGED_PATH = \"./merged_model\"\n\nprint(\"베이스 모델 로드...\")\nbase_model = Qwen3VLForConditionalGeneration.from_pretrained(\n    BASE_MODEL,\n    dtype=torch.bfloat16,\n    device_map=\"cpu\",\n    trust_remote_code=True,\n)\n\nprint(\"LoRA 어댑터 로드...\")\nmodel = PeftModel.from_pretrained(base_model, LORA_ADAPTER)\n\nprint(\"프로세서 로드...\")\nprocessor = AutoProcessor.from_pretrained(BASE_MODEL, trust_remote_code=True)\n\nprint(f\"베이스: {BASE_MODEL}\")\nprint(f\"LoRA: {LORA_ADAPTER}\")\nprint(f\"모델 파라미터: {sum(p.numel() for p in model.parameters()):,}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "베이스 모델 로드...\n",
      "LoRA 어댑터 로드...\n",
      "프로세서 로드...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "video_preprocessor_config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2e2fc5f34f684572bbc85016b3b671e9"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hun41\\miniconda3\\envs\\playground\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hun41\\.cache\\huggingface\\hub\\models--Qwen--Qwen3-VL-2B-Thinking. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "chat_template.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ea0115318c1848ce90c2f080354d36b3"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "베이스: Qwen/Qwen3-VL-2B-Thinking\n",
      "LoRA: adwel94/vision-emoji-recognition-lora\n",
      "모델 파라미터: 2,144,964,608\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-27T06:04:15.411318400Z",
     "start_time": "2026-02-27T06:04:07.579530600Z"
    }
   },
   "source": [
    "print(\"어댑터 머지 중...\")\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "print(f\"머지 후 파라미터: {sum(p.numel() for p in merged_model.parameters()):,}\")\n",
    "\n",
    "print(f\"로컬 저장: {MERGED_PATH}\")\n",
    "merged_model.save_pretrained(MERGED_PATH)\n",
    "processor.save_pretrained(MERGED_PATH)\n",
    "\n",
    "print(\"로컬 저장 완료\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어댑터 머지 중...\n",
      "머지 후 파라미터: 2,127,532,032\n",
      "로컬 저장: ./merged_model\n",
      "로컬 저장 완료\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-27T06:09:16.156218100Z",
     "start_time": "2026-02-27T06:08:18.224261800Z"
    }
   },
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "HF_REPO = \"adwel94/Qwen3-VL-2B-Emoji-Base\"\n",
    "\n",
    "api = HfApi()\n",
    "api.create_repo(HF_REPO, exist_ok=True)\n",
    "\n",
    "print(f\"HF 업로드 시작: {HF_REPO}\")\n",
    "api.upload_folder(\n",
    "    folder_path=MERGED_PATH,\n",
    "    repo_id=HF_REPO,\n",
    "    commit_message=\"Merge emoji recognition LoRA into base model\",\n",
    ")\n",
    "print(f\"업로드 완료: https://huggingface.co/{HF_REPO}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF 업로드 시작: adwel94/Qwen3-VL-2B-Emoji-Base\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1c8390ae80384182bf946e1de58ac15b"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bc3857e9f47f45599dacc44c3624e60e"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "  ...otebook\\merged_model\\tokenizer.json:  73%|#######2  | 8.30MB / 11.4MB            "
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7b68598a6b4c4b96b866956adb71efc6"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "  ...book\\merged_model\\model.safetensors:   1%|          | 41.9MB / 4.26GB            "
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0db9bd7780404d25b51aa190a533b354"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "업로드 완료: https://huggingface.co/adwel94/Qwen3-VL-2B-Emoji-Base\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-27T06:10:21.700425900Z",
     "start_time": "2026-02-27T06:09:24.339922100Z"
    }
   },
   "source": "from transformers import Qwen3VLForConditionalGeneration, AutoProcessor\nimport torch\n\nHF_REPO = \"adwel94/Qwen3-VL-2B-Emoji-Base\"\n\nprint(f\"검증: {HF_REPO} 로드...\")\nverify_model = Qwen3VLForConditionalGeneration.from_pretrained(\n    HF_REPO,\n    dtype=torch.bfloat16,\n    device_map=\"cpu\",\n    trust_remote_code=True,\n)\nverify_processor = AutoProcessor.from_pretrained(HF_REPO, trust_remote_code=True)\n\nprint(f\"파라미터: {sum(p.numel() for p in verify_model.parameters()):,}\")\nprint(\"검증 완료 — 머지 모델 정상 로드\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증: adwel94/Qwen3-VL-2B-Emoji-Base 로드...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ef443466188e4cd18668e5b0d40866d8"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hun41\\miniconda3\\envs\\playground\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\hun41\\.cache\\huggingface\\hub\\models--adwel94--Qwen3-VL-2B-Emoji-Base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.26G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "85947107bc9d4917afefce336156758d"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/204 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7d9e7eea7f5b464b8c16e05c9b5f874b"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/821 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b700d6f182a94fc9801facb6b591abdf"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "24ffc3248cac400a945d92d4081292ce"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "786b0e3861624a6f8be46e6a93629e93"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "10fc2d5dda614c839c24deff6ee33e7e"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8615dae8a8e046c28e4da53597e42645"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/735 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7ad2c6c771cf4c3db8dfa9500986c666"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/644 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "045a5c0582b44533a104d32ff78066fd"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "43302f3fcf5746a9833466febf8e128f"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "video_preprocessor_config.json:   0%|          | 0.00/858 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "68f85edf44a8490d83bacf5c280bd840"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파라미터: 2,127,532,032\n",
      "검증 완료 — 머지 모델 정상 로드\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
