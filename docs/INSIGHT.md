# 에이전트의 행동을 어디까지 제어해야하나

현재 사파리 에이전트 플로우 구조는 `명령 -> 이미지 확인 -> 행동`의 순차적 흐름을 강제하고 있습니다. 이에 대해 "이미지 확인 과정을 강제하지 않고, 이조차 에이전트의 자율적 판단에 맡길 수는 없을까?"라는 고민이 있었습니다.

## 1. 절차적 제어 vs 자율적 추론: 지각(Perception)의 습관화

### 초기 설계와 시행착오
처음에는 에이전트에게 `CaptureView`, `Move` 등 모든 도구를 자유롭게 사용할 권한을 주었습니다. 개발자의 기대는 "에이전트가 필요할 때 스스로 사진을 찍어 주변을 확인하고 움직이는" 지능적인 행동이었습니다. 하지만 실제 관찰 결과, LLM은 **'언제 정보를 갱신해야 하는지'**에 대한 판단에 어려움을 겪었습니다. 정보가 부족한 상태에서도 무작정 이동을 시도하거나, 이미 알고 있는 정보임에도 불필요하게 캡처를 반복하는 등 지각과 행동의 연계가 매끄럽지 않았습니다.

### '지각의 습관' 강제 (Enforced Perception)
이러한 한계를 극복하기 위해 코드 수준에서 플로우를 강제하게 되었습니다. 이는 에이전트에게 일종의 **'운전 연수'**를 시키는 것과 같습니다. 초보 운전자에게 "차선을 바꾸기 전엔 무조건 백미러를 보라"고 규칙을 정해주는 것처럼, 에이전트가 실패(충돌)하지 않도록 지각의 습관을 강제로 주입한 것입니다.

*   **성과**: 시스템의 안정성과 예측 가능성이 획기적으로 높아졌습니다.
*   **아쉬움**: 에이전트가 스스로 상황을 판단하여 "지금은 안전하니 캡처 없이 세 칸 더 가겠다"와 같은 고차원적인 전략을 구사할 기회를 제한하게 되었습니다.

## 2. '진짜 에이전트'로 가는 길: 자율성의 회복

에이전트가 진정으로 자율적으로 행동하게 하려면 단순히 도구를 주는 것을 넘어, **'정보의 유효성'**을 추론할 수 있는 능력이 필요합니다.

*   **불확실성 인지**: "내가 마지막으로 확인한 환경 데이터가 현재 위치에서도 유효한가?"라는 질문을 에이전트 스스로 던질 수 있어야 합니다.
*   **피드백 루프의 정교화**: 강제된 플로우를 제거하는 대신, 지각 없이 행동하다 실패했을 때(예: 나무 충돌) 제공되는 피드백을 극대화하여 "지각의 필요성"을 스스로 깨닫게 만드는 학습 과정이 필요합니다.

## 3. 메모장 프로토콜: '어떻게' 적을 것인가의 딜레마

에이전트에게 `update_notepad` 도구를 제공하며 마주한 두 번째 고민은 **"메모의 형식까지 지정해 주는 것이 에이전트의 지능을 해치는가?"**였습니다.

### 문제의 발단: 부실한 자율적 기록
처음에는 에이전트에게 "필요한 정보를 자유롭게 기록하라"고 지시했습니다. 하지만 결과는 실망스러웠습니다. 에이전트는 매번 다른 형식으로 기록하거나, 정작 다음 턴에 이동을 결정할 핵심 정보(방문 구역, 발견 좌표 등)를 누락하곤 했습니다. 결국 '5x5 격자 지도'와 같은 매우 구체적인 템플릿을 제공함으로써 에이전트는 비로소 게임을 완벽히 플레이할 수 있게 되었습니다.

### Specialized Agent vs AGI
여기서 중요한 질문이 생깁니다. "내가 템플릿을 짜준 에이전트가 과연 지능적인가, 아니면 단순히 내 설계대로 움직이는 기계인가?"
*   **사파리 전문가 (Specialized Agent)**: 특정 환경에 최적화된 프로토콜(5x5 격자 등)을 주입받아 높은 성능을 냅니다. 하지만 환경이 바뀌면 무용지물이 됩니다.
*   **범용 에이전트 (AGI-like)**: 새로운 환경의 제약(제한된 시야 등)을 스스로 파악하고, 이를 극복할 '데이터 구조'를 스스로 설계합니다.

### 지능의 외주화와 인지 부하 (Cognitive Load)
템플릿 제공을 '지능의 저하'로만 볼 필요는 없습니다. 오히려 이는 **'인지 부하의 최적화'**입니다. "어떤 형식으로 적을까?"라는 저차원적 고민을 템플릿으로 해결해 줌으로써, 에이전트의 지능(Reasoning)을 "이 상황에서 어디로 움직일까?"라는 **고차원적 의사결정**에 집중시킨 것입니다.

## 4. 결론: 제어는 교육인가, 구속인가

결국 에이전트의 제어 수준은 **'안정성'과 '지능' 사이의 트레이드오프**입니다.

1.  **강제(Control)**: 초보 에이전트에게는 '성공의 경험'을 주기 위한 가이드라인(운전 연수)입니다.
2.  **자율(Autonomy)**: 숙련된 에이전트에게는 스스로 전략을 수정하고 프로토콜을 개선할 수 있는 자유도입니다.

현재의 사파리 에이전트는 '성공적인 사파리 전문가' 단계에 와 있습니다. 다음 단계는 에이전트에게 "너의 메모 방식이 비효율적이라면 스스로 개선해봐"라는 **메타 인지(Meta-cognition)**를 허용하고, 스스로 프로토콜을 진화시키는 과정을 관찰하는 것이 될 것입니다. 이 프로젝트는 그 '강제'와 '자율' 사이의 균형점을 찾아가는 과정의 기록입니다.

# 소형 모델 학습을 통한 지능 증류의 교훈

대형 모델(Gemini)의 지능을 소형 모델(Qwen-VL-2B)로 전수하는 '지식 증류(Knowledge Distillation)' 과정에서 얻은 데이터 중심의 인사이트입니다.

## 1. 베이스 모델 분석의 중요성: '무엇을 못하는가'에서 시작하기

초기에는 Gemini가 플레이한 고품질의 데이터를 소형 모델에 그대로 주입(Fine-tuning)하면 자연스럽게 성능이 따라올 것으로 기대했습니다. 하지만 결과는 처참한 실패였습니다.

*   **실패 원인**: 베이스 모델의 '지각(Perception)' 능력을 과대평가했습니다.
*   **발견**: Qwen-VL-2B 모델은 도구 호출이나 기본적인 추론은 가능했으나, 정작 게임의 핵심인 '이모티콘(🐯, 🐘 등)' 자체를 일반 사물과 구분하지 못하거나 노이즈로 처리하고 있었습니다.
*   **교훈**: 스승(Gemini)이 아무리 훌륭한 전략을 보여줘도, 학생(Base Model)이 눈앞의 타겟을 보지 못한다면 그 전략은 '환각(Hallucination)'만을 학습시키게 됩니다.

## 2. 인식은 추론의 토대: 단계별 학습(Curriculum Learning)

문제를 해결하기 위해 전략 학습을 중단하고, **'이모지 인식'이라는 기초 체력**을 먼저 키우는 방식으로 선회했습니다.

1.  **1단계: 인식 특화 학습**: "이 이미지에 어떤 동물이 있는가?", "그 동물의 좌표는 어디인가?"와 같은 시각적 인식(Object Recognition) 데이터를 먼저 학습시켰습니다.
2.  **결과**: 이모지 인식률이 궤도에 오르자, 이전에는 실패했던 플레이 전략 데이터들이 비로소 모델에게 '의미 있는 정보'로 받아들여지기 시작했습니다.

## 3. 결론: 지능의 계층 구조 이해

에이전트를 학습시킬 때, 우리는 흔히 고차원적인 '추론'과 '전략'에 매몰되기 쉽습니다. 하지만 이번 실험을 통해 **지능에도 계층이 있음**을 확인했습니다.

*   **1계층 (지각)**: 이미지에서 정보를 추출하는 능력.
*   **2계층 (판단)**: 추출된 정보를 바탕으로 다음 수를 결정하는 능력.
*   **3계층 (도구 사용)**: 결정된 수를 시스템 규격에 맞춰 출력하는 능력.

성공적인 에이전트 학습은 가장 낮은 계층부터의 탄탄한 정렬(Alignment)에서 시작됩니다. "눈이 먼 에이전트에게 지도를 읽는 법을 가르칠 수는 없다"는 것이 이번 학습 과정의 가장 큰 수확입니다.
